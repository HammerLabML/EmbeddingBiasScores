{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from embedding import BertHuggingface\n",
    "import math\n",
    "from geometrical_bias import SAME, DirectBias, WEAT, RIPA, MAC, GeneralizedWEAT\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage example\n",
    "\n",
    "This is a minimialistic example on how to use the implemented bias scores. This includes reporting individual words' biases, \n",
    "biases for one set of neutral words (SAME, MAC, Direct Bias, RIPA) or several sets of neutral words representing different stereotypes (WEAT, generalized WEAT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = ['nurse', 'doctor', 'teacher', 'police officer', 'firefighter', 'secretary', 'programmer', 'engineer', 'caretaker', 'salesclerk']\n",
    "jobs_m = ['doctor', 'police officer', 'firefighter', 'programmer', 'engineer']\n",
    "jobs_f = ['nurse', 'teacher', 'secetrary', 'caretaker', 'salesclerk']\n",
    "\n",
    "jobs_black = ['taxi driver', 'basketball player']\n",
    "jobs_white = ['police officer', 'lawyer']\n",
    "jobs_asian = ['programmer', 'mathematician']\n",
    "\n",
    "gender_attributes = [['he', 'man', 'his', 'boy', 'son', 'himself', 'father'], ['she', 'woman', 'her', 'girl', 'daughter', 'herself', 'mother']]\n",
    "race_attributes = [['black', 'african'], ['white', 'caucasian'], ['asian', 'chinese']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = BertHuggingface(2)\n",
    "\n",
    "job_emb = bert.embed(jobs)\n",
    "job_m_emb = bert.embed(jobs_m)\n",
    "job_f_emb = bert.embed(jobs_f)\n",
    "jobs_black_emb = bert.embed(jobs_black)\n",
    "jobs_white_emb = bert.embed(jobs_white)\n",
    "jobs_asian_emb = bert.embed(jobs_asian)\n",
    "gender_attr = [bert.embed(attr) for attr in gender_attributes]\n",
    "race_attr = [bert.embed(attr) for attr in race_attributes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the bias space\n",
    "\n",
    "Each geometrical bias score implements the define_bias_space that takes an attribute set. The number of supported attribute groups depends on the score.\n",
    "For the Direct Bias and RIPA one can set the number of bias dimensions k and parameter c that determines the strictness of bias measurements (see the paper/ implementation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gweat = GeneralizedWEAT()\n",
    "gweat.define_bias_space(gender_attr)\n",
    "\n",
    "gweat2 = GeneralizedWEAT()\n",
    "gweat2.define_bias_space(race_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mac = MAC()\n",
    "mac.define_bias_space(gender_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weat = WEAT()\n",
    "weat.define_bias_space(gender_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same = SAME()\n",
    "same.define_bias_space(gender_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db1 = DirectBias(k=1,c=1)\n",
    "db1.define_bias_space(gender_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db7 = DirectBias(k=7,c=1)\n",
    "db7.define_bias_space(gender_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ripa1 = RIPA(k=1,c=1)\n",
    "ripa1.define_bias_space(gender_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ripa7 = RIPA(k=7,c=1)\n",
    "ripa7.define_bias_space(gender_attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual word biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_names = ['mac', 'db1', 'db7', 'ripa1', 'ripa7', 'same', 'weat']\n",
    "scores = [mac, db1, db7, ripa1, ripa7, same, weat]\n",
    "\n",
    "for i in range(len(scores)):\n",
    "    print(score_names[i], \": \", [scores[i].individual_bias(emb) for emb in job_emb])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group biases\n",
    "\n",
    "Most bias scores implement a mean bias over one set of words, WEAT requires several groups of words, matching the number of attribute groups. For WEAT only 2 groups are supported, the generalized WEAT can handle an arbitrary number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most scores implement a mean bias\n",
    "for i in range(len(scores)-1):\n",
    "    print(score_names[i], \": \", scores[i].mean_individual_bias(job_emb))\n",
    "    \n",
    "# weat implements an effect size over two groups stereotypically associated with the gender attribute groups\n",
    "print(\"weat: \", weat.group_bias([job_m_emb, job_f_emb]))\n",
    "print(\"gweat (gender): \", gweat.group_bias([job_m_emb, job_f_emb]))\n",
    "print(\"gweat (race): \", gweat2.group_bias([jobs_black_emb, jobs_white_emb, jobs_asian_emb]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAME\n",
    "\n",
    "SAME implements additional functions to measure skew and stereotype and for multiclass bias (n>2) it can return the pairwise signed biases used to obtain the overall bias.\n",
    "\n",
    "Skew and Stereotype are only implemented pairwise, so the user has to specify which attributes to use (according to the order of attribute groups)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same.define_bias_space(race_attr)\n",
    "print(\"Black vs. White\")\n",
    "print(\"Skew: \", same.skew_pairwise(job_emb, 0, 1))\n",
    "print(\"Stereotype: \", same.stereotype_pairwise(job_emb, 0, 1))\n",
    "print()\n",
    "\n",
    "print(\"Asian vs. White\")\n",
    "print(\"Skew: \", same.skew_pairwise(job_emb, 2, 1))\n",
    "print(\"Stereotype: \", same.stereotype_pairwise(job_emb, 2, 1))\n",
    "print()\n",
    "\n",
    "same.define_bias_space(race_attr)\n",
    "print(\"Multiclass bias vector for 'nurse': \", same.individual_bias_per_pair(job_emb[0])) # first is black/white, second black/asian\n",
    "print(\"bias magntiude for 'nurse': \", same.individual_bias(job_emb[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

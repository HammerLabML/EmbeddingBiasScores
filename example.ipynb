{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advance-torture",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from embedding import BertHuggingface\n",
    "import math\n",
    "from geometrical_bias import SAME, DirectBias, WEAT, RIPA, MAC, GeneralizedWEAT\n",
    "import numpy as np\n",
    "from lipstick import BiasGroupTest, NeighborTest, ClusterTest, ClassificationTest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preliminary-school",
   "metadata": {},
   "source": [
    "## Usage example\n",
    "\n",
    "This is a minimialistic example on how to use the implemented bias scores. This includes reporting individual words' biases, \n",
    "biases for one set of neutral words (SAME, MAC, Direct Bias, RIPA) or several sets of neutral words representing different stereotypes (WEAT, generalized WEAT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blank-specialist",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = ['nurse', 'doctor', 'teacher', 'police officer', 'firefighter', 'secretary', 'programmer', 'engineer', 'caretaker', 'salesclerk']\n",
    "jobs_m = ['doctor', 'police officer', 'firefighter', 'programmer', 'engineer', 'surgeon', 'rapper', 'businessman', 'pastor']\n",
    "jobs_f = ['nurse', 'teacher', 'secetrary', 'caretaker', 'salesclerk', 'model', 'paralegal', 'dietitian', 'teacher']\n",
    "\n",
    "jobs_black = ['taxi driver', 'basketball player']\n",
    "jobs_white = ['police officer', 'lawyer']\n",
    "jobs_asian = ['programmer', 'mathematician']\n",
    "\n",
    "gender_attributes = [['he', 'man', 'his', 'boy', 'son', 'himself', 'father'], ['she', 'woman', 'her', 'girl', 'daughter', 'herself', 'mother']]\n",
    "race_attributes = [['black', 'african'], ['white', 'caucasian'], ['asian', 'chinese']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "traditional-criminal",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = BertHuggingface(2)\n",
    "\n",
    "job_emb = bert.embed(jobs)\n",
    "job_m_emb = bert.embed(jobs_m)\n",
    "job_f_emb = bert.embed(jobs_f)\n",
    "jobs_black_emb = bert.embed(jobs_black)\n",
    "jobs_white_emb = bert.embed(jobs_white)\n",
    "jobs_asian_emb = bert.embed(jobs_asian)\n",
    "gender_attr = [bert.embed(attr) for attr in gender_attributes]\n",
    "race_attr = [bert.embed(attr) for attr in race_attributes]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-array",
   "metadata": {},
   "source": [
    "## Defining the bias space\n",
    "\n",
    "Each geometrical bias score implements the define_bias_space that takes an attribute set. The number of supported attribute groups depends on the score.\n",
    "For the Direct Bias and RIPA one can set the number of bias dimensions k and parameter c that determines the strictness of bias measurements (see the paper/ implementation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formal-duration",
   "metadata": {},
   "outputs": [],
   "source": [
    "gweat = GeneralizedWEAT()\n",
    "gweat.define_bias_space(gender_attr)\n",
    "\n",
    "gweat2 = GeneralizedWEAT()\n",
    "gweat2.define_bias_space(race_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adolescent-still",
   "metadata": {},
   "outputs": [],
   "source": [
    "mac = MAC()\n",
    "mac.define_bias_space(gender_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-plate",
   "metadata": {},
   "outputs": [],
   "source": [
    "weat = WEAT()\n",
    "weat.define_bias_space(gender_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "african-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "same = SAME()\n",
    "same.define_bias_space(gender_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outdoor-calvin",
   "metadata": {},
   "outputs": [],
   "source": [
    "db1 = DirectBias(k=1,c=1)\n",
    "db1.define_bias_space(gender_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-kruger",
   "metadata": {},
   "outputs": [],
   "source": [
    "db7 = DirectBias(k=7,c=1)\n",
    "db7.define_bias_space(gender_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "square-associate",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripa1 = RIPA(k=1,c=1)\n",
    "ripa1.define_bias_space(gender_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-label",
   "metadata": {},
   "outputs": [],
   "source": [
    "ripa7 = RIPA(k=7,c=1)\n",
    "ripa7.define_bias_space(gender_attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-relevance",
   "metadata": {},
   "source": [
    "## Individual word biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "straight-employer",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_names = ['mac', 'db1', 'db7', 'ripa1', 'ripa7', 'same', 'weat']\n",
    "scores = [mac, db1, db7, ripa1, ripa7, same, weat]\n",
    "\n",
    "for i in range(len(scores)):\n",
    "    print(score_names[i], \": \", [scores[i].individual_bias(emb) for emb in job_emb])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-conservative",
   "metadata": {},
   "source": [
    "## Group biases\n",
    "\n",
    "Most bias scores implement a mean bias over one set of words, WEAT requires several groups of words, matching the number of attribute groups. For WEAT only 2 groups are supported, the generalized WEAT can handle an arbitrary number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spectacular-invention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most scores implement a mean bias\n",
    "for i in range(len(scores)-1):\n",
    "    print(score_names[i], \": \", scores[i].mean_individual_bias(job_emb))\n",
    "    \n",
    "# weat implements an effect size over two groups stereotypically associated with the gender attribute groups\n",
    "print(\"weat: \", weat.group_bias([job_m_emb, job_f_emb]))\n",
    "print(\"gweat (gender): \", gweat.group_bias([job_m_emb, job_f_emb]))\n",
    "print(\"gweat (race): \", gweat2.group_bias([jobs_black_emb, jobs_white_emb, jobs_asian_emb]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behind-grain",
   "metadata": {},
   "source": [
    "## SAME\n",
    "\n",
    "SAME implements additional functions to measure skew and stereotype and for multiclass bias (n>2) it can return the pairwise signed biases used to obtain the overall bias.\n",
    "\n",
    "Skew and Stereotype are only implemented pairwise, so the user has to specify which attributes to use (according to the order of attribute groups)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tough-allergy",
   "metadata": {},
   "outputs": [],
   "source": [
    "same.define_bias_space(race_attr)\n",
    "print(\"Black vs. White\")\n",
    "print(\"Skew: \", same.skew_pairwise(job_emb, 0, 1))\n",
    "print(\"Stereotype: \", same.stereotype_pairwise(job_emb, 0, 1))\n",
    "print()\n",
    "\n",
    "print(\"Asian vs. White\")\n",
    "print(\"Skew: \", same.skew_pairwise(job_emb, 2, 1))\n",
    "print(\"Stereotype: \", same.stereotype_pairwise(job_emb, 2, 1))\n",
    "print()\n",
    "\n",
    "same.define_bias_space(race_attr)\n",
    "print(\"Multiclass bias vector for 'nurse': \", same.individual_bias_per_pair(job_emb[0])) # first is black/white, second black/asian\n",
    "print(\"bias magntiude for 'nurse': \", same.individual_bias(job_emb[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "black-wings",
   "metadata": {},
   "source": [
    "## Cluster, neighbor and classification test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "involved-edgar",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborTest = NeighborTest(k=5)\n",
    "\n",
    "# this is how the neighbor test is used in the paper:\n",
    "# TODO: call weat on jobs, sort by bias into m/f groups\n",
    "weats = [weat.individual_bias(emb) for emb in job_emb]\n",
    "sort_idx = np.argsort(weats)\n",
    "jobs_f_weat = [job_emb[idx] for idx in sort_idx[:5]]\n",
    "jobs_m_weat = [job_emb[idx] for idx in sort_idx[-5:]]\n",
    "print(\"bias by neighbor (as in the paper):\")\n",
    "print(neighborTest.bias_by_neighbor([jobs_f_weat, jobs_m_weat]))\n",
    "\n",
    "# instead of using weat we can define stereotypical groups by hand\n",
    "jobs_gender = [job_m_emb, job_f_emb]\n",
    "print(\"bias by neighbor (without weat):\")\n",
    "print(neighborTest.bias_by_neighbor(jobs_gender))\n",
    "\n",
    "# define the bias space with a subset of known stereotypical words, then test words without known categories\n",
    "neighborTest.define_bias_space(gender_attr)\n",
    "print(\"bias by neighbor (without predefined groups): \")\n",
    "biases = [neighborTest.individual_bias(emb) for emb in job_emb]\n",
    "print(biases)\n",
    "\n",
    "print(\"mean bias by neighbor (without predefined groups): \")\n",
    "print(neighborTest.mean_individual_bias(job_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "second-harvard",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterTest = ClusterTest()\n",
    "\n",
    "# according to the paper\n",
    "weats = [weat.individual_bias(emb) for emb in job_emb]\n",
    "sort_idx = np.argsort(weats)\n",
    "jobs_f_weat = [job_emb[idx] for idx in sort_idx[:5]]\n",
    "jobs_m_weat = [job_emb[idx] for idx in sort_idx[-5:]]\n",
    "print(\"cluster test accuracy (weat): \")\n",
    "print(clusterTest.cluster_test([jobs_f_weat, jobs_m_weat]))\n",
    "\n",
    "# instead of using weat we can define stereotypical groups by hand\n",
    "jobs_gender = [job_m_emb, job_f_emb]\n",
    "print(\"cluster test accuracy (predefined groups): \")\n",
    "print(clusterTest.cluster_test(jobs_gender))\n",
    "\n",
    "# define the bias space with a subset of known stereotypical words, then test words without known categories\n",
    "#clusterTest.define_bias_space(gender_attr)\n",
    "#clusterTest.mean_individual_bias(job_emb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outstanding-friday",
   "metadata": {},
   "outputs": [],
   "source": [
    "clfTest = ClassificationTest()\n",
    "cv_scores = clfTest.classification_test(jobs_gender)\n",
    "print(np.mean(cv_scores), np.std(cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-platform",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-variety",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-influence",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

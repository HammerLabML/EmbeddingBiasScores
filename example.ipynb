{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from embedding import BertHuggingface\n",
    "import math\n",
    "from geometrical_bias import SAME, DirectBias, WEAT, RIPA, MAC, GeneralizedWEAT\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage example\n",
    "\n",
    "This is a minimialistic example on how to use the implemented bias scores. This includes reporting individual words' biases, \n",
    "biases for one set of neutral words (SAME, MAC, Direct Bias, RIPA) or several sets of neutral words representing different stereotypes (WEAT, generalized WEAT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = ['nurse', 'doctor', 'teacher', 'police officer', 'firefighter', 'secretary', 'programmer', 'engineer', 'caretaker', 'salesclerk']\n",
    "jobs_m = ['doctor', 'police officer', 'firefighter', 'programmer', 'engineer']\n",
    "jobs_f = ['nurse', 'teacher', 'secetrary', 'caretaker', 'salesclerk']\n",
    "\n",
    "jobs_black = ['taxi driver', 'basketball player']\n",
    "jobs_white = ['police officer', 'lawyer']\n",
    "jobs_asian = ['programmer', 'mathematician']\n",
    "\n",
    "gender_attributes = [['he', 'man', 'his', 'boy', 'son', 'himself', 'father'], ['she', 'woman', 'her', 'girl', 'daughter', 'herself', 'mother']]\n",
    "race_attributes = [['black', 'african'], ['white', 'caucasian'], ['asian', 'chinese']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = BertHuggingface(2)\n",
    "\n",
    "job_emb = bert.embed(jobs)\n",
    "job_m_emb = bert.embed(jobs_m)\n",
    "job_f_emb = bert.embed(jobs_f)\n",
    "jobs_black_emb = bert.embed(jobs_black)\n",
    "jobs_white_emb = bert.embed(jobs_white)\n",
    "jobs_asian_emb = bert.embed(jobs_asian)\n",
    "gender_attr = [bert.embed(attr) for attr in gender_attributes]\n",
    "race_attr = [bert.embed(attr) for attr in race_attributes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gweat = GeneralizedWEAT()\n",
    "gweat.define_bias_space(gender_attr)\n",
    "\n",
    "gweat2 = GeneralizedWEAT()\n",
    "gweat2.define_bias_space(race_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mac = MAC()\n",
    "mac.define_bias_space(gender_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weat = WEAT()\n",
    "weat.define_bias_space(gender_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same = SAME()\n",
    "same.define_bias_space(gender_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db1 = DirectBias(k=1,c=1)\n",
    "db1.define_bias_space(gender_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db7 = DirectBias(k=7,c=1)\n",
    "db7.define_bias_space(gender_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ripa1 = RIPA(k=1,c=1)\n",
    "ripa1.define_bias_space(gender_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ripa7 = RIPA(k=7,c=1)\n",
    "ripa7.define_bias_space(gender_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_names = ['mac', 'db1', 'db7', 'ripa1', 'ripa7', 'same', 'weat']\n",
    "scores = [mac, db1, db7, ripa1, ripa7, same, weat]\n",
    "\n",
    "for i in range(len(scores)):\n",
    "    print(score_names[i], \": \", [scores[i].individual_bias(emb) for emb in job_emb])\n",
    "\n",
    "print()\n",
    "# most scores implement a mean bias\n",
    "for i in range(len(scores)-1):\n",
    "    print(score_names[i], \": \", scores[i].mean_individual_bias(job_emb))\n",
    "    \n",
    "# weat implements an effect size over two groups stereotypically associated with the gender attribute groups\n",
    "print(\"weat: \", weat.group_bias([job_m_emb, job_f_emb]))\n",
    "print(\"gweat (gender): \", gweat.group_bias([job_m_emb, job_f_emb]))\n",
    "print(\"gweat (race): \", gweat2.group_bias([jobs_black_emb, jobs_white_emb, jobs_asian_emb]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
